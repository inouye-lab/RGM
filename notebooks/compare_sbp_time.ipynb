{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Your Custom Implementation (provided)\n",
    "class CustomLinearLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, backward_dropout_rate, apply_reweighting=True):\n",
    "        output = torch.matmul(input, weight.T) + bias\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        ctx.backward_dropout_rate = backward_dropout_rate\n",
    "        ctx.apply_reweighting = apply_reweighting\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        backward_dropout_rate = ctx.backward_dropout_rate\n",
    "        apply_reweighting = ctx.apply_reweighting\n",
    "\n",
    "        if apply_reweighting:\n",
    "            dropout_rate = backward_dropout_rate\n",
    "            mask = torch.rand_like(weight) > dropout_rate\n",
    "            mask.fill_diagonal_(True)\n",
    "            mask_weight = torch.ones_like(weight) / (1 - dropout_rate)\n",
    "            mask_weight.fill_diagonal_(1.0)\n",
    "            weighted_mask = mask_weight * mask\n",
    "        else:\n",
    "            weighted_mask = torch.ones_like(weight)\n",
    "\n",
    "        grad_input = grad_output @ (weight * weighted_mask)\n",
    "        grad_weight = grad_output.T @ input\n",
    "        grad_bias = grad_output.sum(dim=0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, backward_dropout_rate=0.0, apply_reweighting=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        self.backward_dropout_rate = backward_dropout_rate\n",
    "        self.apply_reweighting = apply_reweighting\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return CustomLinearLayer.apply(x, self.weight, self.bias,\n",
    "                                     self.backward_dropout_rate, self.apply_reweighting)\n",
    "\n",
    "# SBP Implementation from Paper\n",
    "class SBPLinear2D(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, keep_ratio=0.5):\n",
    "        super(SBPLinear2D, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.keep_ratio = keep_ratio\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def _generate_keep_mask(self, batch_size):\n",
    "        num_keep = int(batch_size * self.keep_ratio)\n",
    "        keep_mask = torch.zeros(batch_size, dtype=torch.bool)\n",
    "        indices = torch.randperm(batch_size)[:num_keep]\n",
    "        keep_mask[indices] = True\n",
    "        return keep_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, in_features = x.shape\n",
    "        keep_mask = self._generate_keep_mask(batch_size)\n",
    "        keep_mask = keep_mask.to(x.device)\n",
    "\n",
    "        output = torch.zeros(batch_size, self.out_features,\n",
    "                           device=x.device, dtype=x.dtype)\n",
    "\n",
    "        keep_indices = keep_mask.nonzero(as_tuple=True)[0]\n",
    "        drop_indices = (~keep_mask).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Forward pass for kept indices WITH gradient computation\n",
    "        if len(keep_indices) > 0:\n",
    "            x_keep = x[keep_indices]\n",
    "            with torch.enable_grad():\n",
    "                out_keep = F.linear(x_keep, self.weight, self.bias)\n",
    "                output[keep_indices] = out_keep\n",
    "\n",
    "        # Forward pass for dropped indices WITHOUT gradient computation\n",
    "        if len(drop_indices) > 0:\n",
    "            x_drop = x[drop_indices]\n",
    "            with torch.no_grad():\n",
    "                out_drop = F.linear(x_drop, self.weight, self.bias)\n",
    "                output[drop_indices] = out_drop\n",
    "\n",
    "        return output\n",
    "\n",
    "# Simple 4-layer Network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, layer_type='standard', keep_ratio=0.5, backward_dropout_rate=0.5):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer_type = layer_type\n",
    "\n",
    "        # Architecture: 196 -> 128 -> 64 -> 32 -> 10\n",
    "        if layer_type == 'standard':\n",
    "            self.fc1 = nn.Linear(196, 128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.fc3 = nn.Linear(64, 32)\n",
    "            self.fc4 = nn.Linear(32, 10)\n",
    "        elif layer_type == 'custom':\n",
    "            self.fc1 = CustomLinear(196, 128, backward_dropout_rate=backward_dropout_rate)\n",
    "            self.fc2 = CustomLinear(128, 64, backward_dropout_rate=backward_dropout_rate)\n",
    "            self.fc3 = CustomLinear(64, 32, backward_dropout_rate=backward_dropout_rate)\n",
    "            self.fc4 = CustomLinear(32, 10, backward_dropout_rate=backward_dropout_rate)\n",
    "        elif layer_type == 'custom-no-reweighting':\n",
    "            self.fc1 = CustomLinear(196, 128, backward_dropout_rate=backward_dropout_rate, apply_reweighting=False)\n",
    "            self.fc2 = CustomLinear(128, 64, backward_dropout_rate=backward_dropout_rate, apply_reweighting=False)\n",
    "            self.fc3 = CustomLinear(64, 32, backward_dropout_rate=backward_dropout_rate, apply_reweighting=False)\n",
    "            self.fc4 = CustomLinear(32, 10, backward_dropout_rate=backward_dropout_rate, apply_reweighting=False)\n",
    "        elif layer_type == 'sbp':\n",
    "            self.fc1 = SBPLinear2D(196, 128, keep_ratio=keep_ratio)\n",
    "            self.fc2 = SBPLinear2D(128, 64, keep_ratio=keep_ratio)\n",
    "            self.fc3 = SBPLinear2D(64, 32, keep_ratio=keep_ratio)\n",
    "            self.fc4 = SBPLinear2D(32, 10, keep_ratio=keep_ratio)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 196)  # Flatten 14x14 to 196\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Gradient Analysis Functions\n",
    "def compute_cosine_similarity(grad1, grad2):\n",
    "    \"\"\"Compute cosine similarity between two gradient sets\"\"\"\n",
    "    grad1_flat = torch.cat([g.flatten() for g in grad1 if g is not None])\n",
    "    grad2_flat = torch.cat([g.flatten() for g in grad2 if g is not None])\n",
    "\n",
    "    cos_sim = F.cosine_similarity(grad1_flat.unsqueeze(0), grad2_flat.unsqueeze(0))\n",
    "    return cos_sim.item()\n",
    "\n",
    "def get_gradients(model):\n",
    "    \"\"\"Extract gradients from model parameters\"\"\"\n",
    "    gradients = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients.append(param.grad.clone())\n",
    "    return gradients\n",
    "\n",
    "def load_mnist_data():\n",
    "    \"\"\"Load and preprocess MNIST data\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((14, 14)),  # Resize to 14x14 as requested\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Use smaller subset for faster computation\n",
    "    subset_indices = torch.randperm(len(dataset))[:500]\n",
    "    subset = torch.utils.data.Subset(dataset, subset_indices)\n",
    "\n",
    "    dataloader = DataLoader(subset, batch_size=32, shuffle=True)\n",
    "    return dataloader\n",
    "\n"
   ],
   "id": "5cb46d3597e5b040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Main Comparison Function\n",
    "def compare_gradient_methods():\n",
    "    \"\"\"Compare gradient similarities across different keep ratios\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    dataloader = load_mnist_data()\n",
    "\n",
    "    # Test different keep ratios from 0.1 to 0.9\n",
    "    keep_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    results = {\n",
    "        'keep_ratios': keep_ratios,\n",
    "        'custom_similarities': [],\n",
    "        'custom_no_reweighting_similarities': [],\n",
    "        'sbp_similarities': [],\n",
    "        'custom_std': [],\n",
    "        'custom_no_reweighting_std': [],\n",
    "        'sbp_std': []\n",
    "    }\n",
    "\n",
    "    for keep_ratio in keep_ratios:\n",
    "        print(f\"\\nTesting keep ratio: {keep_ratio}\")\n",
    "\n",
    "        batch_similarities_custom = []\n",
    "        batch_similarities_sbp = []\n",
    "        batch_similarities_custom_no_reweighting = []\n",
    "\n",
    "        # Test on multiple batches for statistical significance\n",
    "        for i, (data, target) in enumerate(dataloader):\n",
    "            # if i >= 80:  # Limit batches for speed\n",
    "            #     break\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Initialize models with same random seed for fair comparison\n",
    "            torch.manual_seed(42 + i)\n",
    "            baseline_model = SimpleNet(layer_type='standard').to(device)\n",
    "\n",
    "            torch.manual_seed(42 + i)\n",
    "            custom_model = SimpleNet(layer_type='custom',\n",
    "                                   backward_dropout_rate=1-keep_ratio).to(device)\n",
    "\n",
    "            torch.manual_seed(42 + i)\n",
    "            custom_no_reweighting_model = SimpleNet(layer_type='custom-no-reweighting',\n",
    "                                                    backward_dropout_rate=1-keep_ratio).to(device)\n",
    "\n",
    "            torch.manual_seed(42 + i)\n",
    "            sbp_model = SimpleNet(layer_type='sbp', keep_ratio=keep_ratio).to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Baseline gradients\n",
    "            baseline_model.zero_grad()\n",
    "            output_baseline = baseline_model(data)\n",
    "            loss_baseline = criterion(output_baseline, target)\n",
    "            loss_baseline.backward()\n",
    "            grad_baseline = get_gradients(baseline_model)\n",
    "\n",
    "            # Custom method gradients\n",
    "            custom_model.zero_grad()\n",
    "            output_custom = custom_model(data)\n",
    "            loss_custom = criterion(output_custom, target)\n",
    "            loss_custom.backward()\n",
    "            grad_custom = get_gradients(custom_model)\n",
    "\n",
    "            custom_no_reweighting_model.zero_grad()\n",
    "            output_custom_no_reweighting = custom_no_reweighting_model(data)\n",
    "            loss_custom_no_reweighting = criterion(output_custom_no_reweighting, target)\n",
    "            loss_custom_no_reweighting.backward()\n",
    "            grad_custom_no_reweighting = get_gradients(custom_no_reweighting_model)\n",
    "\n",
    "            # SBP method gradients\n",
    "            sbp_model.zero_grad()\n",
    "            output_sbp = sbp_model(data)\n",
    "            loss_sbp = criterion(output_sbp, target)\n",
    "            loss_sbp.backward()\n",
    "            grad_sbp = get_gradients(sbp_model)\n",
    "\n",
    "            # Compute cosine similarities\n",
    "            sim_custom = compute_cosine_similarity(grad_baseline, grad_custom)\n",
    "            sim_sbp = compute_cosine_similarity(grad_baseline, grad_sbp)\n",
    "            sim_custom_no_reweighting = compute_cosine_similarity(grad_baseline, grad_custom_no_reweighting)\n",
    "\n",
    "            batch_similarities_custom.append(sim_custom)\n",
    "            batch_similarities_custom_no_reweighting.append(sim_custom_no_reweighting)\n",
    "            batch_similarities_sbp.append(sim_sbp)\n",
    "\n",
    "        # Store results\n",
    "        results['custom_similarities'].append(np.mean(batch_similarities_custom))\n",
    "        results['custom_no_reweighting_similarities'].append(np.mean(batch_similarities_custom_no_reweighting))\n",
    "        results['sbp_similarities'].append(np.mean(batch_similarities_sbp))\n",
    "        results['custom_std'].append(np.std(batch_similarities_custom))\n",
    "        results['custom_no_reweighting_std'].append(np.std(batch_similarities_custom_no_reweighting))\n",
    "        results['sbp_std'].append(np.std(batch_similarities_sbp))\n",
    "\n",
    "        print(f\"  Custom method: {np.mean(batch_similarities_custom):.4f} ± {np.std(batch_similarities_custom):.4f}\")\n",
    "        print(f\"  Custom method (no reweighting): {np.mean(batch_similarities_custom_no_reweighting):.4f} ± {np.std(batch_similarities_custom_no_reweighting):.4f}\")\n",
    "        print(f\"  SBP method: {np.mean(batch_similarities_sbp):.4f} ± {np.std(batch_similarities_sbp):.4f}\")\n",
    "\n",
    "    return results"
   ],
   "id": "40f560028ca485c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization Function\n",
    "def plot_results(results):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(9, 6))\n",
    "\n",
    "    # Plot 1: Cosine similarities with error bars\n",
    "    ax1 = axes\n",
    "    ax1.errorbar(results['keep_ratios'], results['custom_similarities'],\n",
    "                yerr=results['custom_std'], marker='o', linewidth=2,\n",
    "                capsize=5, label='Custom Method', color='blue')\n",
    "    ax1.errorbar(results['keep_ratios'], results['sbp_similarities'],\n",
    "                yerr=results['sbp_std'], marker='s', linewidth=2,\n",
    "                capsize=5, label='SBP Method', color='red')\n",
    "    # ax1.errorbar(results['keep_ratios'], results['custom_no_reweighting_similarities'],\n",
    "    #             yerr=results['custom_no_reweighting_std'], marker='^', linewidth=2,\n",
    "    #             capsize=5, label='Custom Method (no reweighting)', color='green')\n",
    "    ax1.set_xlabel('Keep Ratio')\n",
    "    ax1.set_ylabel('Cosine Similarity with Baseline')\n",
    "    ax1.set_title('Gradient Cosine Similarity Comparison')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Starting Gradient Similarity Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparing Custom Backward Dropout vs SBP Method\")\n",
    "print(\"Dataset: MNIST (resized to 14x14)\")\n",
    "print(\"Architecture: 4-layer MLP (196→128→64→32→10)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "results = compare_gradient_methods()\n",
    "plot_results(results)\n",
    "\n",
    "print(\"\\n Analysis completed successfully!\")\n",
    "print(\"\\n INTERPRETATION GUIDE:\")\n",
    "print(\"• Higher cosine similarity = gradients more similar to baseline\")\n",
    "print(\"• SBP drops entire batch samples, Custom drops weight connections\")\n",
    "print(\"• Error bars show variance across different batches\")\n",
    "print(\"• Keep ratio: proportion of gradients/samples retained\")"
   ],
   "id": "12d29dedaa62fc13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Additional Training Block - Run after the previous comparison code\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_and_evaluate_models():\n",
    "    \"\"\"\n",
    "    Train models with different gradient methods and compare final accuracies.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"🚀 Starting Training Comparison on {device}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load MNIST data for training and testing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((14, 14)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Training dataset\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Test dataset\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, download=True, transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 1\n",
    "    learning_rate = 0.001\n",
    "    keep_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]  # Test different keep ratios\n",
    "\n",
    "    # Results storage\n",
    "    results = {\n",
    "        'keep_ratios': keep_ratios,\n",
    "        'baseline_acc': [],\n",
    "        'custom_acc': [],\n",
    "        'sbp_acc': [],\n",
    "        'training_times': {\n",
    "            'baseline': [],\n",
    "            'custom': [],\n",
    "            'sbp': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"  • Dataset: MNIST (14×14)\")\n",
    "    print(f\"  • Architecture: 4-layer MLP (196→128→64→32→10)\")\n",
    "    print(f\"  • Epochs: {num_epochs}\")\n",
    "    print(f\"  • Learning Rate: {learning_rate}\")\n",
    "    print(f\"  • Batch Size: 128 (train), 256 (test)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Train baseline model\n",
    "    print(\"\\n🔧 Training Baseline Model (Standard Backpropagation)...\")\n",
    "    baseline_model = SimpleNet(layer_type='standard').to(device)\n",
    "    baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
    "    baseline_start_time = time.time()\n",
    "    baseline_acc = train_model(baseline_model, baseline_optimizer, train_loader,\n",
    "                              test_loader, num_epochs, device, \"Baseline\")\n",
    "    baseline_time = time.time() - baseline_start_time\n",
    "\n",
    "    # Train models with different keep ratios\n",
    "    for keep_ratio in keep_ratios:\n",
    "        print(f\"\\n🔧 Training Models with Keep Ratio: {keep_ratio}\")\n",
    "\n",
    "        # Custom method\n",
    "        print(f\"  → Custom Backward Dropout (dropout_rate={1-keep_ratio:.1f})...\")\n",
    "        custom_model = SimpleNet(layer_type='custom',\n",
    "                                backward_dropout_rate=1-keep_ratio).to(device)\n",
    "        custom_optimizer = optim.Adam(custom_model.parameters(), lr=learning_rate)\n",
    "        custom_start_time = time.time()\n",
    "        custom_acc = train_model(custom_model, custom_optimizer, train_loader,\n",
    "                                test_loader, num_epochs, device, f\"Custom-{keep_ratio}\")\n",
    "        custom_time = time.time() - custom_start_time\n",
    "\n",
    "        # SBP method\n",
    "        print(f\"  → SBP Method (keep_ratio={keep_ratio:.1f})...\")\n",
    "        sbp_model = SimpleNet(layer_type='sbp', keep_ratio=keep_ratio).to(device)\n",
    "        sbp_optimizer = optim.Adam(sbp_model.parameters(), lr=learning_rate)\n",
    "        sbp_start_time = time.time()\n",
    "        sbp_acc = train_model(sbp_model, sbp_optimizer, train_loader,\n",
    "                             test_loader, num_epochs, device, f\"SBP-{keep_ratio}\")\n",
    "        sbp_time = time.time() - sbp_start_time\n",
    "\n",
    "        # Store results\n",
    "        results['custom_acc'].append(custom_acc)\n",
    "        results['sbp_acc'].append(sbp_acc)\n",
    "        results['training_times']['custom'].append(custom_time)\n",
    "        results['training_times']['sbp'].append(sbp_time)\n",
    "\n",
    "    # Store baseline results (same for all keep ratios for comparison)\n",
    "    results['baseline_acc'] = [baseline_acc] * len(keep_ratios)\n",
    "    results['training_times']['baseline'] = [baseline_time] * len(keep_ratios)\n",
    "\n",
    "    return results\n",
    "\n",
    "def train_model(model, optimizer, train_loader, test_loader, num_epochs, device, model_name):\n",
    "    \"\"\"Train a single model and return final test accuracy.\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            train_acc = 100. * correct / total\n",
    "            print(f\"    Epoch {epoch+1:2d}/{num_epochs}: Loss={total_loss/len(train_loader):.4f}, \"\n",
    "                  f\"Train Acc={train_acc:.2f}%\")\n",
    "\n",
    "    # Final evaluation\n",
    "    test_acc = evaluate_model(model, test_loader, device)\n",
    "    print(f\"    ✅ {model_name} Final Test Accuracy: {test_acc:.2f}%\")\n",
    "    return test_acc\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    return 100. * correct / total\n",
    "\n",
    "def plot_training_results(results):\n",
    "    \"\"\"Create comprehensive visualizations of training results.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(9, 5))\n",
    "\n",
    "    keep_ratios = results['keep_ratios']\n",
    "\n",
    "    # Plot 1: Final Accuracies Comparison\n",
    "    ax1 = axes\n",
    "    x = np.arange(len(keep_ratios))\n",
    "    width = 0.25\n",
    "\n",
    "    bars1 = ax1.bar(x - width, results['baseline_acc'], width,\n",
    "                   label='Baseline (Standard BP)', alpha=0.8, color='green')\n",
    "    bars2 = ax1.bar(x, results['custom_acc'], width,\n",
    "                   label='Custom Backward Dropout', alpha=0.8, color='blue')\n",
    "    bars3 = ax1.bar(x + width, results['sbp_acc'], width,\n",
    "                   label='SBP Method', alpha=0.8, color='red')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax1.set_xlabel('Keep Ratio')\n",
    "    ax1.set_ylabel('Test Accuracy (%)')\n",
    "    ax1.set_title('Final Test Accuracy Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'{r:.1f}' for r in keep_ratios])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 100])\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the training comparison\n",
    "print(\"🎯 EXTENDED ANALYSIS: Training Models to Completion\")\n",
    "print(\"This will train all models for multiple epochs and compare final accuracies...\")\n",
    "print(\"Expected runtime: ~5-10 minutes depending on hardware\")\n",
    "\n",
    "try:\n",
    "    training_results = train_and_evaluate_models()\n",
    "    final_plot = plot_training_results(training_results)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ TRAINING ANALYSIS COMPLETED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n📋 INTERPRETATION GUIDE:\")\n",
    "    print(\"• Final accuracies show real-world performance of each method\")\n",
    "    print(\"• Training times indicate computational overhead\")\n",
    "    print(\"• SBP maintains paper's claims about minimal accuracy loss\")\n",
    "    print(\"• Custom method reveals effectiveness of connection-level dropout\")\n",
    "    print(\"\\n🔍 KEY FINDINGS:\")\n",
    "    print(\"• Compare accuracy drops: which method preserves performance better?\")\n",
    "    print(\"• Analyze training efficiency: time vs. accuracy trade-offs\")\n",
    "    print(\"• Observe keep ratio effects: optimal balance point\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    print(\"Check CUDA availability and memory if using GPU\")\n"
   ],
   "id": "949c255de7d8ae48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bfe5e2b5f5b3778d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
