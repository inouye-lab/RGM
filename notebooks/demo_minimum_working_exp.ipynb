{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## frameworks",
   "id": "68701c2ace8b3ac6"
  },
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import gc\n",
    "\n",
    "# Function level Code but with F.linear\n",
    "class OriginalDropoutFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, keep_ratio=0.5):\n",
    "        # --- Full Forward Pass ---\n",
    "        output = F.linear(input, weight)\n",
    "        # --- Memory-Saving \"Backdrop\" ---\n",
    "        # Now, create the reduced tensors that you will need for the backward pass.\n",
    "        num_cols_to_keep = int(weight.size(1) * keep_ratio)\n",
    "        kept_cols_indices = torch.randperm(weight.size(1), device=input.device)[:num_cols_to_keep]\n",
    "        input_reduced = input[:, kept_cols_indices]\n",
    "        # Save only the reduced input and other necessary components.\n",
    "        # We also need the original weight to recompute the reduced weight in backward.\n",
    "        ctx.save_for_backward(input_reduced, weight)\n",
    "        ctx.kept_cols_indices = kept_cols_indices\n",
    "        ctx.input_shape = input.shape # Needed for reconstructing grad_input\n",
    "        return output\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if not ctx.needs_input_grad[0] and not ctx.needs_input_grad[1]:\n",
    "            return None, None, None\n",
    "        input_reduced, weight = ctx.saved_tensors\n",
    "        kept_cols_indices = ctx.kept_cols_indices\n",
    "        grad_input = grad_weight = None\n",
    "        # --- Recomputation Step ---\n",
    "        # Re-create the reduced weight tensor on-the-fly.\n",
    "        weight_reduced = weight[:, kept_cols_indices]\n",
    "        # --- Gradient Calculation ---\n",
    "        # Now, compute gradients using the reduced tensors.\n",
    "        if ctx.needs_input_grad[1]: # Gradient for weight\n",
    "            grad_weight_reduced = grad_output.t().mm(input_reduced)\n",
    "            grad_weight = torch.zeros_like(weight)\n",
    "            grad_weight[:, kept_cols_indices] = grad_weight_reduced\n",
    "        if ctx.needs_input_grad[0]: # Gradient for input\n",
    "            grad_input_reduced = grad_output.mm(weight_reduced)\n",
    "            grad_input = torch.zeros(ctx.input_shape, device=grad_output.device)\n",
    "            grad_input[:, kept_cols_indices] = grad_input_reduced\n",
    "        return grad_input, grad_weight, None\n",
    "\n",
    "# Function level Code but with F.linear surrounded by torch.no_grad()\n",
    "class FixedDropoutFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, keep_ratio=0.5):\n",
    "        # --- Full Forward Pass ---\n",
    "        with torch.no_grad():\n",
    "            output = F.linear(input, weight)\n",
    "        # --- Memory-Saving \"Backdrop\" ---\n",
    "        # Now, create the reduced tensors that you will need for the backward pass.\n",
    "        num_cols_to_keep = int(weight.size(1) * keep_ratio)\n",
    "        kept_cols_indices = torch.randperm(weight.size(1), device=input.device)[:num_cols_to_keep]\n",
    "        input_reduced = input[:, kept_cols_indices]\n",
    "        # Save only the reduced input and other necessary components.\n",
    "        # We also need the original weight to recompute the reduced weight in backward.\n",
    "        ctx.save_for_backward(input_reduced, weight)\n",
    "        ctx.kept_cols_indices = kept_cols_indices\n",
    "        ctx.input_shape = input.shape # Needed for reconstructing grad_input\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        if not ctx.needs_input_grad[0] and not ctx.needs_input_grad[1]:\n",
    "            return None, None, None\n",
    "        input_reduced, weight = ctx.saved_tensors\n",
    "        kept_cols_indices = ctx.kept_cols_indices\n",
    "        grad_input = grad_weight = None\n",
    "        # --- Recomputation Step ---\n",
    "        # Re-create the reduced weight tensor on-the-fly.\n",
    "        weight_reduced = weight[:, kept_cols_indices]\n",
    "        # --- Gradient Calculation ---\n",
    "        # Now, compute gradients using the reduced tensors.\n",
    "        if ctx.needs_input_grad[1]: # Gradient for weight\n",
    "            grad_weight_reduced = grad_output.t().mm(input_reduced)\n",
    "            grad_weight = torch.zeros_like(weight)\n",
    "            grad_weight[:, kept_cols_indices] = grad_weight_reduced\n",
    "        if ctx.needs_input_grad[0]: # Gradient for input\n",
    "            grad_input_reduced = grad_output.mm(weight_reduced)\n",
    "            grad_input = torch.zeros(ctx.input_shape, device=grad_output.device)\n",
    "            grad_input[:, kept_cols_indices] = grad_input_reduced\n",
    "        return grad_input, grad_weight, None\n",
    "\n",
    "# nn.Module level Code in sbp-style code\n",
    "class BackdropLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, keep_ratio=0.5, bias=True):\n",
    "        super(BackdropLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.keep_ratio = keep_ratio\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        effective_fan_in = fan_in * self.keep_ratio\n",
    "\n",
    "        gain = nn.init.calculate_gain('leaky_relu', math.sqrt(5))\n",
    "        std = gain / math.sqrt(effective_fan_in)\n",
    "        bound = math.sqrt(3.0) * std\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            bound = 1 / math.sqrt(effective_fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        kept_cols = torch.rand(self.in_features, device=input.device) < self.keep_ratio\n",
    "        kept_cols = kept_cols.bool()\n",
    "        # drop_cols = ~kept_cols\n",
    "\n",
    "        # input_kept = input[:, kept_cols]\n",
    "        # input_drop = input[:, drop_cols]\n",
    "\n",
    "        output = F.linear(input[:, kept_cols], self.weight[:, kept_cols], self.bias)\n",
    "        with torch.no_grad():\n",
    "            output += F.linear(input[:, ~kept_cols], self.weight[:, ~kept_cols], self.bias)\n",
    "\n",
    "        return output"
   ],
   "id": "642ece5669f9c92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class ColumnwiseStructuredBackDropLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, keep_ratio, backdrop_method='fixed', bias=True, init_adjusted=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.keep_ratio = keep_ratio\n",
    "\n",
    "        if init_adjusted:\n",
    "            self.reset_parameters_adjusted()\n",
    "        else:\n",
    "            self.reset_parameters()\n",
    "\n",
    "        if backdrop_method == 'fixed':\n",
    "            self.backdrop_function = FixedDropoutFunction\n",
    "        elif backdrop_method == 'original':\n",
    "            self.backdrop_method = OriginalDropoutFunction\n",
    "        else:\n",
    "            raise ValueError(\"Invalid backdrop_method. Choose 'fixed' or 'original'.\")\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / (fan_in ** 0.5)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def reset_parameters_adjusted(self):\n",
    "        # Adjusted Kaiming initialization for partial updates\n",
    "        fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "\n",
    "        # Calculate effective fan_in based on keep_ratio\n",
    "        effective_fan_in = fan_in * self.keep_ratio\n",
    "\n",
    "        # Kaiming uniform with adjusted fan_in\n",
    "        gain = nn.init.calculate_gain('leaky_relu', math.sqrt(5))\n",
    "        std = gain / math.sqrt(effective_fan_in)\n",
    "        bound = math.sqrt(3.0) * std\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "        # Adjusted bias initialization\n",
    "        if self.bias is not None:\n",
    "            bound = 1 / math.sqrt(effective_fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input, training=True):\n",
    "        return self.backdrop_function.apply(input, self.weight, self.bias, training, self.keep_ratio)"
   ],
   "id": "3c34144be1f537b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple Model",
   "id": "64d58d083f15739b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTestModel(nn.Module):\n",
    "    # The 'use_backdrop' argument is now correctly included in the constructor\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, keep_ratio=0.5, backdrop_method=\"No_drop\", device='cpu'):\n",
    "        super().__init__()\n",
    "        self.keep_ratio = keep_ratio\n",
    "        self.backdrop_method = backdrop_method\n",
    "\n",
    "        if self.backdrop_method == \"No_drop\":\n",
    "            self.layer1 = nn.Linear(input_dim, hidden_dim).to(device)\n",
    "            self.layer2 = nn.Linear(hidden_dim, output_dim).to(device)\n",
    "        elif self.backdrop_method == \"fixed\":\n",
    "            self.layer1 = ColumnwiseStructuredBackDropLinear(input_dim, hidden_dim, keep_ratio=keep_ratio, backdrop_method='fixed', init_adjusted=True).to(device)\n",
    "            self.layer2 = ColumnwiseStructuredBackDropLinear(hidden_dim, output_dim, keep_ratio=keep_ratio, backdrop_method='fixed', init_adjusted=True).to(device)\n",
    "        elif self.backdrop_method == \"original\":\n",
    "            self.layer1 = ColumnwiseStructuredBackDropLinear(input_dim, hidden_dim, keep_ratio=keep_ratio, backdrop_method='original', init_adjusted=True).to(device)\n",
    "            self.layer2 = ColumnwiseStructuredBackDropLinear(hidden_dim, output_dim, keep_ratio=keep_ratio, backdrop_method='original', init_adjusted=True).to(device)\n",
    "        elif self.backdrop_method == \"sbp-style\":\n",
    "            self.layer1 = BackdropLinear(input_dim, hidden_dim, keep_ratio=keep_ratio).to(device)\n",
    "            self.layer2 = BackdropLinear(hidden_dim, output_dim, keep_ratio=keep_ratio).to(device)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid backdrop_method. Choose 'No_drop', 'Function_fixed', 'Function_original', or 'sbp-style'.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x"
   ],
   "id": "d2d538899eb47895",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Profiling",
   "id": "bb5968af4b8b20d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def profile_model(model, inputs, label, device):\n",
    "    # Ensure all tensors for the training step are on the correct device\n",
    "    model.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Use a simple loss function and optimizer for a realistic training step\n",
    "    model.train()\n",
    "    gc.collect()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    targets = torch.randint(0, 10, (inputs.shape[0],), device=device)\n",
    "\n",
    "    # --- UPDATED TRACE HANDLER ---\n",
    "    # Define the trace handler to save both the Chrome trace and the memory timeline\n",
    "    def trace_handler(p):\n",
    "        # Export the standard Chrome trace for operator/kernel timelines\n",
    "        chrome_trace_filename = f\"./{label}_trace.json\"\n",
    "        p.export_chrome_trace(chrome_trace_filename)\n",
    "        print(f\"\\nTrace for '{label}' saved to {chrome_trace_filename}\")\n",
    "\n",
    "        # Export the memory timeline to a separate HTML file [11]\n",
    "        memory_timeline_filename = f\"./{label}_memory_timeline.html\"\n",
    "        p.export_memory_timeline(memory_timeline_filename, device=device)\n",
    "        print(f\"Memory timeline for '{label}' saved to {memory_timeline_filename}\")\n",
    "\n",
    "    # Configure and run the profiler\n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        profile_memory=True,\n",
    "        record_shapes=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=trace_handler\n",
    "    ) as prof:\n",
    "        for _ in range(3): # Run a couple of steps for warmup and profiling\n",
    "            with record_function(\"forward_pass\"):\n",
    "                outputs = model(inputs)\n",
    "\n",
    "            with record_function(\"loss_computation\"):\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "            with record_function(\"backward_pass\"):\n",
    "                loss.backward()\n",
    "\n",
    "            with record_function(\"optimizer_step\"):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            prof.step() # Signal the profiler to record the step"
   ],
   "id": "e98f566dec50aea9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running the profiling",
   "id": "763978b73f88e674"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Model parameters\n",
    "    batch_size = int(102400)\n",
    "    input_dim = 1024\n",
    "    hidden_dim = 4096\n",
    "    output_dim = 1024\n",
    "    keep_ratio = 0.1\n",
    "    device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Profile the standard model ---\n",
    "    print(\"Profiling standard nn.Linear model...\")\n",
    "    standard_model = SimpleTestModel(input_dim, hidden_dim, output_dim, backdrop_method=\"No_drop\", device=device)\n",
    "    standard_model = torch.compile(standard_model, fullgraph=True, mode=\"reduce-overhead\")\n",
    "    inputs = torch.randn(batch_size, input_dim)\n",
    "    profile_model(standard_model, inputs, \"standard_model\", device)\n",
    "\n",
    "    # # --- Profile the backdrop model ---\n"
   ],
   "id": "6148c3f8b7aec294"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
