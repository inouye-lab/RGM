{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Your Custom Implementation (provided)\n",
    "class RGMLinearLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, backward_dropout_rate, apply_reweighting=True):\n",
    "        output = torch.matmul(input, weight.T) + bias\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        ctx.backward_dropout_rate = backward_dropout_rate\n",
    "        ctx.apply_reweighting = apply_reweighting\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        backward_dropout_rate = ctx.backward_dropout_rate\n",
    "        apply_reweighting = ctx.apply_reweighting\n",
    "        dropout_rate = backward_dropout_rate\n",
    "\n",
    "        # Create dropout mask, ensuring diagonal is always kept\n",
    "        mask = torch.rand_like(weight) > dropout_rate\n",
    "        mask.fill_diagonal_(True)\n",
    "\n",
    "        if apply_reweighting:\n",
    "            mask_weight = torch.ones_like(weight) / (1 - dropout_rate)\n",
    "            mask_weight.fill_diagonal_(1.0)\n",
    "            weighted_mask = mask_weight * mask\n",
    "        else:\n",
    "            mask_weight = torch.ones_like(weight)\n",
    "            # mask_weight.fill_diagonal_(1.0)\n",
    "            weighted_mask = mask_weight * mask\n",
    "\n",
    "        grad_input = grad_output @ (weight * weighted_mask)\n",
    "        grad_weight = grad_output.T @ input\n",
    "        grad_bias = grad_output.sum(dim=0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None\n",
    "\n",
    "\n",
    "class RGMLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, backward_dropout_rate=0.0, apply_reweighting=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        self.backward_dropout_rate = backward_dropout_rate\n",
    "        self.apply_reweighting = apply_reweighting\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return RGMLinearLayer.apply(x, self.weight, self.bias,\n",
    "                                     self.backward_dropout_rate, self.apply_reweighting)\n",
    "\n",
    "# SBP Implementation from Paper\n",
    "class SBPLinear2D(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, keep_ratio=0.5):\n",
    "        super(SBPLinear2D, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.keep_ratio = keep_ratio\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def _generate_keep_mask(self, batch_size):\n",
    "        num_keep = int(batch_size * self.keep_ratio)\n",
    "        keep_mask = torch.zeros(batch_size, dtype=torch.bool)\n",
    "        indices = torch.randperm(batch_size)[:num_keep]\n",
    "        keep_mask[indices] = True\n",
    "        return keep_mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, in_features = x.shape\n",
    "        keep_mask = self._generate_keep_mask(batch_size)\n",
    "        keep_mask = keep_mask.to(x.device)\n",
    "\n",
    "        output = torch.zeros(batch_size, self.out_features,\n",
    "                           device=x.device, dtype=x.dtype)\n",
    "\n",
    "        keep_indices = keep_mask.nonzero(as_tuple=True)[0]\n",
    "        drop_indices = (~keep_mask).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Forward pass for kept indices WITH gradient computation\n",
    "        if len(keep_indices) > 0:\n",
    "            x_keep = x[keep_indices]\n",
    "            with torch.enable_grad():\n",
    "                out_keep = F.linear(x_keep, self.weight, self.bias)\n",
    "            output[keep_indices] = out_keep\n",
    "\n",
    "        # Forward pass for dropped indices WITHOUT gradient computation\n",
    "        if len(drop_indices) > 0:\n",
    "            x_drop = x[drop_indices]\n",
    "            with torch.no_grad():\n",
    "                out_drop = F.linear(x_drop, self.weight, self.bias)\n",
    "            output[drop_indices] = out_drop\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=196,\n",
    "                 output_dim=10,\n",
    "                 hidden_dims=[128, 64, 32],\n",
    "                 layer_type='standard',\n",
    "                 keep_ratio=0.5,\n",
    "                 activation='relu'):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer_type = layer_type\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Build the complete architecture dimensions\n",
    "        self.layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "\n",
    "        # Create layers based on architecture\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(self.layer_dims) - 1):\n",
    "            in_dim = self.layer_dims[i]\n",
    "            out_dim = self.layer_dims[i + 1]\n",
    "\n",
    "            if layer_type == 'standard':\n",
    "                layer = nn.Linear(in_dim, out_dim)\n",
    "            elif layer_type == 'rgm':\n",
    "                layer = RGMLinear(in_dim, out_dim, backward_dropout_rate=1-keep_ratio)\n",
    "            elif layer_type == 'rgm-no-reweighting':\n",
    "                layer = RGMLinear(in_dim, out_dim, backward_dropout_rate=1-keep_ratio, apply_reweighting=False)\n",
    "            elif layer_type == 'sbp':\n",
    "                layer = SBPLinear2D(in_dim, out_dim, keep_ratio=keep_ratio)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer_type: {layer_type}\")\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Set activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input to match input_dim\n",
    "        x = x.view(-1, self.input_dim)\n",
    "\n",
    "        # Forward through all layers except the last one with activation\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "\n",
    "        # Final layer without activation\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "    def get_architecture_info(self):\n",
    "        \"\"\"Return information about the network architecture\"\"\"\n",
    "        return {\n",
    "            'layer_dims': self.layer_dims,\n",
    "            'layer_type': self.layer_type,\n",
    "            'num_layers': len(self.layers),\n",
    "            'total_params': sum(p.numel() for p in self.parameters())\n",
    "        }\n",
    "\n",
    "def get_gradients(model):\n",
    "    \"\"\"Extract gradients from model parameters\"\"\"\n",
    "    gradients = []\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients.append(param.grad.clone())\n",
    "    return gradients\n",
    "\n",
    "\n",
    "def train_and_evaluate_models_time_and_space_analysis(config=None):\n",
    "    \"\"\"\n",
    "    Train models with different gradient methods and compare final accuracies.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary with training parameters\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        'num_epochs': 1,\n",
    "        'learning_rate': 0.001,\n",
    "        'keep_ratios': [0.3, 0.5, 0.7],\n",
    "        'batch_size_train': 1024,\n",
    "        'batch_size_test': 1024,\n",
    "        'image_size': (14, 14),\n",
    "        'dataset': 'MNIST',  # Currently only MNIST supported\n",
    "        'normalize_mean': (0.1307,),\n",
    "        'normalize_std': (0.3081,),\n",
    "        'clear_memory_between_models': True,\n",
    "        'verbose': True,\n",
    "        'optimizer': 'adam',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "        'optimizer_params': {}  # New: additional optimizer parameters\n",
    "    }\n",
    "\n",
    "    # Merge user config with defaults\n",
    "    if config is None:\n",
    "        config = default_config\n",
    "    else:\n",
    "        for key, value in default_config.items():\n",
    "            if key not in config:\n",
    "                config[key] = value\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if config['verbose']:\n",
    "        print(f\"🚀 Starting Training Comparison on {device}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Load data based on configuration\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config['image_size']),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(config['normalize_mean'], config['normalize_std'])\n",
    "    ])\n",
    "\n",
    "    # Training dataset\n",
    "    if config['dataset'] == 'MNIST':\n",
    "        train_dataset = torchvision.datasets.MNIST(\n",
    "            root='./data', train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.MNIST(\n",
    "            root='./data', train=False, download=True, transform=transform\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {config['dataset']} not supported yet\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size_train'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size_test'], shuffle=False)\n",
    "\n",
    "    # Results storage - enhanced to include memory and time data\n",
    "    results = {\n",
    "        'config': config,\n",
    "        'keep_ratios': config['keep_ratios'],\n",
    "        'models': {\n",
    "            'baseline': {},\n",
    "            'custom': {},\n",
    "            'sbp': {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Helper function to create optimizer\n",
    "    def create_optimizer(model_parameters):\n",
    "        optimizer_name = config['optimizer'].lower()\n",
    "        base_params = {'lr': config['learning_rate']}\n",
    "        base_params.update(config['optimizer_params'])\n",
    "\n",
    "        if optimizer_name == 'adam':\n",
    "            return optim.Adam(model_parameters, **base_params)\n",
    "        elif optimizer_name == 'sgd':\n",
    "            return optim.SGD(model_parameters, **base_params)\n",
    "        elif optimizer_name == 'rmsprop':\n",
    "            return optim.RMSprop(model_parameters, **base_params)\n",
    "        elif optimizer_name == 'adamw':\n",
    "            return optim.AdamW(model_parameters, **base_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer '{optimizer_name}' not supported. Use 'adam', 'sgd', 'rmsprop', or 'adamw'\")\n",
    "\n",
    "    if config['verbose']:\n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  • Dataset: {config['dataset']} ({config['image_size'][0]}×{config['image_size'][1]})\")\n",
    "        print(f\"  • Architecture: 4-layer MLP (196→128→64→32→10)\")\n",
    "        print(f\"  • Epochs: {config['num_epochs']}\")\n",
    "        print(f\"  • Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"  • Optimizer: {config['optimizer'].upper()}\")\n",
    "        print(f\"  • Batch Size: {config['batch_size_train']} (train), {config['batch_size_test']} (test)\")\n",
    "        print(f\"  • Keep Ratios: {config['keep_ratios']}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Helper function to clear memory if configured\n",
    "    def clear_memory_if_needed():\n",
    "        if config['clear_memory_between_models'] and device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "            if config['verbose']:\n",
    "                print(\"🧹 GPU memory cleared\")\n",
    "\n",
    "    # Train baseline model\n",
    "    if config['verbose']:\n",
    "        print(\"\\n🔧 Training Baseline Model (Standard Backpropagation)...\")\n",
    "\n",
    "    clear_memory_if_needed()\n",
    "    baseline_model = SimpleNet(layer_type='standard').to(device)\n",
    "    baseline_optimizer = create_optimizer(baseline_model.parameters())\n",
    "\n",
    "    baseline_results = train_model(\n",
    "        model=baseline_model,\n",
    "        optimizer=baseline_optimizer,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        device=device,\n",
    "        model_name=\"Baseline\",\n",
    "        clear_memory_first=False  # Already cleared above\n",
    "    )\n",
    "\n",
    "    results['models']['baseline'] = baseline_results\n",
    "\n",
    "    # Train models with different keep ratios\n",
    "    for keep_ratio in config['keep_ratios']:\n",
    "        if config['verbose']:\n",
    "            print(f\"\\n🔧 Training Models with Keep Ratio: {keep_ratio}\")\n",
    "\n",
    "        # Custom method\n",
    "        if config['verbose']:\n",
    "            print(f\"  → Reweighted Gradient Message (Keep Ratio={keep_ratio:.1f})...\")\n",
    "\n",
    "        clear_memory_if_needed()\n",
    "        custom_model = SimpleNet(layer_type='rgm', keep_ratio=keep_ratio).to(device)\n",
    "        custom_optimizer = create_optimizer(custom_model.parameters())\n",
    "\n",
    "        custom_results = train_model(\n",
    "            model=custom_model,\n",
    "            optimizer=custom_optimizer,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=device,\n",
    "            model_name=f\"Reweighted Gradient Message-{keep_ratio}\",\n",
    "            clear_memory_first=False  # Already cleared above\n",
    "        )\n",
    "\n",
    "        # SBP method\n",
    "        if config['verbose']:\n",
    "            print(f\"  → SBP Method (keep_ratio={keep_ratio:.1f})...\")\n",
    "\n",
    "        clear_memory_if_needed()\n",
    "        sbp_model = SimpleNet(layer_type='sbp', keep_ratio=keep_ratio).to(device)\n",
    "        sbp_optimizer = create_optimizer(sbp_model.parameters())\n",
    "\n",
    "        sbp_results = train_model(\n",
    "            model=sbp_model,\n",
    "            optimizer=sbp_optimizer,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=device,\n",
    "            model_name=f\"SBP-{keep_ratio}\",\n",
    "            clear_memory_first=False  # Already cleared above\n",
    "        )\n",
    "\n",
    "        # Store results with keep_ratio as key\n",
    "        if 'custom' not in results['models']:\n",
    "            results['models']['custom'] = {}\n",
    "        if 'sbp' not in results['models']:\n",
    "            results['models']['sbp'] = {}\n",
    "\n",
    "        results['models']['custom'][keep_ratio] = custom_results\n",
    "        results['models']['sbp'][keep_ratio] = sbp_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    return 100. * correct / total\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, train_loader, test_loader, num_epochs, device, model_name,\n",
    "                clear_memory_first=True, track_time=False, track_memory=False):\n",
    "    \"\"\"Train a single model and return results dictionary with optional time, VRAM usage, and accuracy tracking.\"\"\"\n",
    "\n",
    "    # Clear GPU memory before starting if requested\n",
    "    if clear_memory_first and device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize time tracking variables\n",
    "    if track_time:\n",
    "        start_time = time.time()\n",
    "\n",
    "    # GPU memory tracking if available and requested\n",
    "    if track_memory and device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        initial_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB\n",
    "        max_gpu_memory = initial_gpu_memory\n",
    "    else:\n",
    "        initial_gpu_memory = 0\n",
    "        max_gpu_memory = 0\n",
    "\n",
    "    # Lists to store epoch-wise metrics\n",
    "    epoch_losses = []\n",
    "    epoch_test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            # Track GPU memory usage only if requested\n",
    "            if track_memory and device.type == 'cuda':\n",
    "                current_gpu_memory = torch.cuda.memory_allocated(device) / 1024 / 1024  # MB\n",
    "                max_gpu_memory = max(max_gpu_memory, current_gpu_memory)\n",
    "\n",
    "        # Store epoch metrics\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        # Evaluate on test set for this epoch\n",
    "        test_acc = evaluate_model(model, test_loader, device)\n",
    "        epoch_test_accuracies.append(test_acc)\n",
    "\n",
    "        # Print progress every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            train_acc = 100. * correct / total\n",
    "            print(f\" Epoch {epoch+1:2d}/{num_epochs}: Loss={avg_loss:.4f}, \"\n",
    "                  f\"Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
    "\n",
    "    # Calculate total training time only if tracking is enabled\n",
    "    if track_time:\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "    # Final evaluation\n",
    "    final_test_acc = epoch_test_accuracies[-1]  # Use last epoch's test accuracy\n",
    "    print(f\" ✅ {model_name} Final Test Accuracy: {final_test_acc:.2f}%\")\n",
    "\n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'test_accuracy': final_test_acc,\n",
    "        'epoch_losses': epoch_losses,\n",
    "        'epoch_test_accuracies': epoch_test_accuracies,\n",
    "    }\n",
    "\n",
    "    # Add time tracking results if enabled\n",
    "    if track_time:\n",
    "        results.update({\n",
    "            'training_time_seconds': training_time,\n",
    "            'training_time_minutes': training_time / 60,\n",
    "        })\n",
    "\n",
    "    # Add GPU memory stats if available and tracking is enabled\n",
    "    if track_memory and device.type == 'cuda':\n",
    "        peak_gpu_memory = torch.cuda.max_memory_allocated(device) / 1024 / 1024  # MB\n",
    "        results['gpu_memory_usage'] = {\n",
    "            'initial_gpu_memory_mb': initial_gpu_memory,\n",
    "            'max_gpu_memory_mb': max_gpu_memory,\n",
    "            'peak_gpu_memory_mb': peak_gpu_memory,\n",
    "            'gpu_memory_increase_mb': peak_gpu_memory - initial_gpu_memory\n",
    "        }\n",
    "        print(f\" 🎮 GPU memory usage: {peak_gpu_memory:.2f}MB (increase: {peak_gpu_memory-initial_gpu_memory:.2f}MB)\")\n",
    "    elif track_memory and device.type != 'cuda':\n",
    "        print(f\" ⚠️ GPU not available - no VRAM tracking\")\n",
    "\n",
    "    # Print time summary only if tracking is enabled\n",
    "    if track_time:\n",
    "        print(f\" ⏱️ Training time: {training_time:.2f}s ({training_time/60:.2f}m)\")\n",
    "\n",
    "    # Optional: Clear memory after training for next model\n",
    "    if device.type == 'cuda':\n",
    "        del model, optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_training_results_time_and_space_analysis(results):\n",
    "    \"\"\"Create comprehensive visualizations of training results.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    keep_ratios = results['keep_ratios']\n",
    "    x = np.arange(len(keep_ratios))\n",
    "    width = 0.25\n",
    "\n",
    "    # Extract accuracy data from new results structure\n",
    "    baseline_acc = [results['models']['baseline']['test_accuracy']] * len(keep_ratios)\n",
    "    custom_acc = [results['models']['custom'][kr]['test_accuracy'] for kr in keep_ratios]\n",
    "    sbp_acc = [results['models']['sbp'][kr]['test_accuracy'] for kr in keep_ratios]\n",
    "\n",
    "    # Extract VRAM data from new results structure (convert MB to GB)\n",
    "    baseline_vram = []\n",
    "    custom_vram = []\n",
    "    sbp_vram = []\n",
    "\n",
    "    for kr in keep_ratios:\n",
    "        # Baseline VRAM (same for all keep ratios)\n",
    "        if 'gpu_memory_usage' in results['models']['baseline']:\n",
    "            baseline_vram.append(results['models']['baseline']['gpu_memory_usage']['peak_gpu_memory_mb'] / 1024)\n",
    "        else:\n",
    "            baseline_vram.append(0)\n",
    "\n",
    "        # Custom VRAM (varies by keep ratio)\n",
    "        if 'gpu_memory_usage' in results['models']['custom'][kr]:\n",
    "            custom_vram.append(results['models']['custom'][kr]['gpu_memory_usage']['peak_gpu_memory_mb'] / 1024)\n",
    "        else:\n",
    "            custom_vram.append(0)\n",
    "\n",
    "        # SBP VRAM (varies by keep ratio)\n",
    "        if 'gpu_memory_usage' in results['models']['sbp'][kr]:\n",
    "            sbp_vram.append(results['models']['sbp'][kr]['gpu_memory_usage']['peak_gpu_memory_mb'] / 1024)\n",
    "        else:\n",
    "            sbp_vram.append(0)\n",
    "\n",
    "    # Plot 1: Final Accuracies Comparison\n",
    "    ax1 = axes[0]\n",
    "\n",
    "    bars1 = ax1.bar(x - width, baseline_acc, width,\n",
    "                   label='Baseline (Standard BP)', alpha=0.8, color='green')\n",
    "    bars2 = ax1.bar(x, custom_acc, width,\n",
    "                   label='Custom Backward Dropout', alpha=0.8, color='blue')\n",
    "    bars3 = ax1.bar(x + width, sbp_acc, width,\n",
    "                   label='SBP Method', alpha=0.8, color='red')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax1.set_xlabel('Keep Ratio')\n",
    "    ax1.set_ylabel('Test Accuracy (%)')\n",
    "    ax1.set_title('Final Test Accuracy Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'{r:.1f}' for r in keep_ratios])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0, 100])\n",
    "\n",
    "    # Plot 2: VRAM Usage Comparison\n",
    "    ax2 = axes[1]\n",
    "\n",
    "    bars1_vram = ax2.bar(x - width, baseline_vram, width,\n",
    "                        label='Baseline (Standard BP)', alpha=0.8, color='green')\n",
    "    bars2_vram = ax2.bar(x, custom_vram, width,\n",
    "                        label='Custom Backward Dropout', alpha=0.8, color='blue')\n",
    "    bars3_vram = ax2.bar(x + width, sbp_vram, width,\n",
    "                        label='SBP Method', alpha=0.8, color='red')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1_vram, bars2_vram, bars3_vram]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:  # Only show label if VRAM tracking is available\n",
    "                ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.2f}GB', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax2.set_xlabel('Keep Ratio')\n",
    "    ax2.set_ylabel('Max VRAM Usage (GB)')\n",
    "    ax2.set_title('Maximum VRAM Usage Comparison')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f'{r:.1f}' for r in keep_ratios])\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# config = {\n",
    "#     'num_epochs': 1,\n",
    "#     'learning_rate': 0.001,\n",
    "#     'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "#     'batch_size_train': 1024,\n",
    "#     'batch_size_test': 1024,\n",
    "#     'clear_memory_between_models': True,\n",
    "#     'verbose': True,\n",
    "#     'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "#     'optimizer_params': {}  # New: additional optimizer parameters\n",
    "# }\n",
    "#\n",
    "# training_results = train_and_evaluate_models_time_and_space_analysis(config)\n",
    "# final_plot = plot_training_results_time_and_space_analysis(training_results)"
   ],
   "id": "b30e106b3bb0f18f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_and_evaluate_models(config=None):\n",
    "    \"\"\"\n",
    "    Train models with different gradient methods and compare final accuracies with plots.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Configuration dictionary with training parameters\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        'num_epochs': 20,  # Changed from 1 to 20 for better plotting\n",
    "        'learning_rate': 0.001,\n",
    "        'keep_ratios': [0.3, 0.5, 0.7],\n",
    "        'batch_size_train': 1024,\n",
    "        'batch_size_test': 1024,\n",
    "        'image_size': (14, 14),\n",
    "        'dataset': 'MNIST',\n",
    "        'normalize_mean': (0.1307,),\n",
    "        'normalize_std': (0.3081,),\n",
    "        'clear_memory_between_models': True,\n",
    "        'verbose': True,\n",
    "        'optimizer': 'adam',\n",
    "        'optimizer_params': {}\n",
    "    }\n",
    "\n",
    "    # Merge user config with defaults\n",
    "    if config is None:\n",
    "        config = default_config\n",
    "    else:\n",
    "        for key, value in default_config.items():\n",
    "            if key not in config:\n",
    "                config[key] = value\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if config['verbose']:\n",
    "        print(f\"🚀 Starting Training Comparison on {device}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Load data based on configuration\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config['image_size']),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(config['normalize_mean'], config['normalize_std'])\n",
    "    ])\n",
    "\n",
    "    # Training dataset\n",
    "    if config['dataset'] == 'MNIST':\n",
    "        train_dataset = torchvision.datasets.MNIST(\n",
    "            root='./data', train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.MNIST(\n",
    "            root='./data', train=False, download=True, transform=transform\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {config['dataset']} not supported yet\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size_train'], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size_test'], shuffle=False)\n",
    "\n",
    "    # Results storage\n",
    "    results = {\n",
    "        'config': config,\n",
    "        'keep_ratios': config['keep_ratios'],\n",
    "        'models': {\n",
    "            'baseline': {},\n",
    "            'rgm': {},\n",
    "            'rgm-no-reweighting': {},\n",
    "            'sbp': {}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Helper function to create optimizer\n",
    "    def create_optimizer(model_parameters):\n",
    "        optimizer_name = config['optimizer'].lower()\n",
    "        base_params = {'lr': config['learning_rate']}\n",
    "        base_params.update(config['optimizer_params'])\n",
    "\n",
    "        if optimizer_name == 'adam':\n",
    "            return optim.Adam(model_parameters, **base_params)\n",
    "        elif optimizer_name == 'sgd':\n",
    "            return optim.SGD(model_parameters, **base_params)\n",
    "        elif optimizer_name == 'rmsprop':\n",
    "            return optim.RMSprop(model_parameters, **base_params)\n",
    "        elif optimizer_name == 'adamw':\n",
    "            return optim.AdamW(model_parameters, **base_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer '{optimizer_name}' not supported. Use 'adam', 'sgd', 'rmsprop', or 'adamw'\")\n",
    "\n",
    "    if config['verbose']:\n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  • Dataset: {config['dataset']} ({config['image_size'][0]}×{config['image_size'][1]})\")\n",
    "        print(f\"  • Architecture: 4-layer MLP (196→128→64→32→10)\")\n",
    "        print(f\"  • Epochs: {config['num_epochs']}\")\n",
    "        print(f\"  • Learning Rate: {config['learning_rate']}\")\n",
    "        print(f\"  • Optimizer: {config['optimizer'].upper()}\")\n",
    "        print(f\"  • Batch Size: {config['batch_size_train']} (train), {config['batch_size_test']} (test)\")\n",
    "        print(f\"  • Keep Ratios: {config['keep_ratios']}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Helper function to clear memory if configured\n",
    "    def clear_memory_if_needed():\n",
    "        if config['clear_memory_between_models'] and device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "            if config['verbose']:\n",
    "                print(\"🧹 GPU memory cleared\")\n",
    "\n",
    "    # Train baseline model\n",
    "    if config['verbose']:\n",
    "        print(\"\\n🔧 Training Baseline Model (Standard Backpropagation)...\")\n",
    "\n",
    "    clear_memory_if_needed()\n",
    "    baseline_model = SimpleNet(layer_type='standard').to(device)\n",
    "    baseline_optimizer = create_optimizer(baseline_model.parameters())\n",
    "\n",
    "    baseline_results = train_model(\n",
    "        model=baseline_model,\n",
    "        optimizer=baseline_optimizer,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        device=device,\n",
    "        model_name=\"Baseline\",\n",
    "        clear_memory_first=False,\n",
    "        track_time=False,  # Disable time tracking\n",
    "        track_memory=False  # Disable memory tracking\n",
    "    )\n",
    "\n",
    "    results['models']['baseline'] = baseline_results\n",
    "\n",
    "    # Train models with different keep ratios\n",
    "    for keep_ratio in config['keep_ratios']:\n",
    "        if config['verbose']:\n",
    "            print(f\"\\n🔧 Training Models with Keep Ratio: {keep_ratio}\")\n",
    "\n",
    "        # RGM method\n",
    "        if config['verbose']:\n",
    "            print(f\"  → Reweighted Gradient Message (Keep Ratio={keep_ratio:.1f})...\")\n",
    "\n",
    "        clear_memory_if_needed()\n",
    "        rgm_model = SimpleNet(layer_type='rgm', keep_ratio=keep_ratio).to(device)\n",
    "        rgm_optimizer = create_optimizer(rgm_model.parameters())\n",
    "\n",
    "        rgm_results = train_model(\n",
    "            model=rgm_model,\n",
    "            optimizer=rgm_optimizer,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=device,\n",
    "            model_name=f\"Reweighted Gradient Message-{keep_ratio}\",\n",
    "            clear_memory_first=False,\n",
    "            track_time=False,  # Disable time tracking\n",
    "            track_memory=False  # Disable memory tracking\n",
    "        )\n",
    "\n",
    "        # Custom method\n",
    "        if config['verbose']:\n",
    "            print(f\"  → Reweighted Gradient Message w/o Reweighting (Keep Ratio={keep_ratio:.1f})...\")\n",
    "\n",
    "        clear_memory_if_needed()\n",
    "        rgm_wo_model = SimpleNet(layer_type='rgm-no-reweighting', keep_ratio=keep_ratio).to(device)\n",
    "        rgm_wo_optimizer = create_optimizer(rgm_wo_model.parameters())\n",
    "\n",
    "        rgm_wo_results = train_model(\n",
    "            model=rgm_wo_model,\n",
    "            optimizer=rgm_wo_optimizer,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=device,\n",
    "            model_name=f\"Reweighted Gradient Message w/o Reweighting-{keep_ratio}\",\n",
    "            clear_memory_first=False,\n",
    "            track_time=False,  # Disable time tracking\n",
    "            track_memory=False  # Disable memory tracking\n",
    "        )\n",
    "\n",
    "        # SBP method\n",
    "        if config['verbose']:\n",
    "            print(f\"  → SBP Method (keep_ratio={keep_ratio:.1f})...\")\n",
    "\n",
    "        clear_memory_if_needed()\n",
    "        sbp_model = SimpleNet(layer_type='sbp', keep_ratio=keep_ratio).to(device)\n",
    "        sbp_optimizer = create_optimizer(sbp_model.parameters())\n",
    "\n",
    "        sbp_results = train_model(\n",
    "            model=sbp_model,\n",
    "            optimizer=sbp_optimizer,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            device=device,\n",
    "            model_name=f\"SBP-{keep_ratio}\",\n",
    "            clear_memory_first=False,\n",
    "            track_time=False,  # Disable time tracking\n",
    "            track_memory=False  # Disable memory tracking\n",
    "        )\n",
    "\n",
    "        # Store results with keep_ratio as key\n",
    "        if 'rgm' not in results['models']:\n",
    "            results['models']['rgm'] = {}\n",
    "        if 'rgm-no-reweighting' not in results['models']:\n",
    "            results['models']['rgm-no-reweighting'] = {}\n",
    "        if 'sbp' not in results['models']:\n",
    "            results['models']['sbp'] = {}\n",
    "\n",
    "        results['models']['rgm'][keep_ratio] = rgm_results\n",
    "        results['models']['rgm-no-reweighting'][keep_ratio] = rgm_wo_results\n",
    "        results['models']['sbp'][keep_ratio] = sbp_results\n",
    "\n",
    "    # Create plots\n",
    "    _plot_training_curves(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _plot_training_curves(results):\n",
    "    \"\"\"Create loss and test accuracy plots for different methods.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot 1: Training Loss\n",
    "    epochs = range(1, len(results['models']['baseline']['epoch_losses']) + 1)\n",
    "\n",
    "    # Baseline\n",
    "    ax1.plot(epochs, results['models']['baseline']['epoch_losses'],\n",
    "             label='Baseline', linewidth=2, marker='o', markersize=4)\n",
    "\n",
    "    # RGM methods\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['rgm']:\n",
    "            ax1.plot(epochs, results['models']['rgm'][keep_ratio]['epoch_losses'],\n",
    "                     label=f'RGM (keep={keep_ratio})', linewidth=2, marker='s', markersize=4)\n",
    "\n",
    "    # RGM without reweighting methods\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['rgm-no-reweighting']:\n",
    "            ax1.plot(epochs, results['models']['rgm-no-reweighting'][keep_ratio]['epoch_losses'],\n",
    "                     label=f'RGM w/o Reweighting (keep={keep_ratio})', linewidth=2, marker='x', markersize=4)\n",
    "\n",
    "    # SBP methods\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['sbp']:\n",
    "            ax1.plot(epochs, results['models']['sbp'][keep_ratio]['epoch_losses'],\n",
    "                     label=f'SBP (keep={keep_ratio})', linewidth=2, marker='^', markersize=4)\n",
    "\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Training Loss')\n",
    "    ax1.set_title('Training Loss Over Epochs')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Test Accuracy\n",
    "    ax2.plot(epochs, results['models']['baseline']['epoch_test_accuracies'],\n",
    "             label='Baseline', linewidth=2, marker='o', markersize=4)\n",
    "\n",
    "    # RGM methods\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['rgm']:\n",
    "            ax2.plot(epochs, results['models']['rgm'][keep_ratio]['epoch_test_accuracies'],\n",
    "                     label=f'RGM (keep={keep_ratio})', linewidth=2, marker='s', markersize=4)\n",
    "\n",
    "    # RGM without reweighting methods\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['rgm-no-reweighting']:\n",
    "            ax2.plot(epochs, results['models']['rgm-no-reweighting'][keep_ratio]['epoch_test_accuracies'],\n",
    "                     label=f'RGM w/o Reweighting (keep={keep_ratio})', linewidth=2, marker='x', markersize=4)\n",
    "\n",
    "    # SBP methods\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['sbp']:\n",
    "            ax2.plot(epochs, results['models']['sbp'][keep_ratio]['epoch_test_accuracies'],\n",
    "                     label=f'SBP (keep={keep_ratio})', linewidth=2, marker='^', markersize=4)\n",
    "\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy (%)')\n",
    "    ax2.set_title('Test Accuracy Over Epochs')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Method':<25} {'Final Test Acc':<15} {'Final Loss':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    baseline = results['models']['baseline']\n",
    "    print(f\"{'Baseline':<25} {baseline['test_accuracy']:<15.2f} {baseline['epoch_losses'][-1]:<12.4f}\")\n",
    "\n",
    "    for keep_ratio in results['keep_ratios']:\n",
    "        if keep_ratio in results['models']['rgm']:\n",
    "            custom = results['models']['rgm'][keep_ratio]\n",
    "            print(f\"{f'RGM (keep={keep_ratio})':<25} {custom['test_accuracy']:<15.2f} {custom['epoch_losses'][-1]:<12.4f}\")\n",
    "\n",
    "        if keep_ratio in results['models']['rgm-no-reweighting']:\n",
    "            custom_wo = results['models']['rgm-no-reweighting'][keep_ratio]\n",
    "            print(f\"{f'RGM w/o Reweighting (keep={keep_ratio})':<25} {custom_wo['test_accuracy']:<15.2f} {custom_wo['epoch_losses'][-1]:<12.4f}\")\n",
    "\n",
    "        if keep_ratio in results['models']['sbp']:\n",
    "            sbp = results['models']['sbp'][keep_ratio]\n",
    "            print(f\"{f'SBP (keep={keep_ratio})':<25} {sbp['test_accuracy']:<15.2f} {sbp['epoch_losses'][-1]:<12.4f}\")\n"
   ],
   "id": "d6aea7485cdf5b5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.5],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "457be4074d3c4633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.1,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.5],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "451bd2ac54b40d30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.1,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.1],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "e836db6eed7ae79d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.01,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.1],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "9a006ea3786ad14a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.1,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.3],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "1a0c46db26c4bf1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.1,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.7],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "f9986bf3beeb88b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.1,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.9],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'sgd',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "2420a455f9c56d4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.01,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.1],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'adam',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "c741695395baa015",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.01,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.3],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'adam',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "842caa1f868f7724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.01,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.5],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'adam',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "80d46f43573cc714",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.01,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.7],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'adam',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "8846b5311ef997b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    'num_epochs': 25,\n",
    "    'learning_rate': 0.01,\n",
    "    # 'keep_ratios': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    'keep_ratios': [0.9],\n",
    "    'batch_size_train': 1024,\n",
    "    'batch_size_test': 1024,\n",
    "    'clear_memory_between_models': True,\n",
    "    'verbose': True,\n",
    "    'optimizer': 'adam',  # New: optimizer type ('adam', 'sgd', 'rmsprop', 'adamw')\n",
    "    'optimizer_params': {}  # New: additional optimizer parameters\n",
    "}\n",
    "\n",
    "training_results_epochs = train_and_evaluate_models(config)"
   ],
   "id": "fbb7b6c40eb08352",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
